# Comparing `tmp/slideflow-2.0.2.post1-py3-none-any.whl.zip` & `tmp/slideflow-2.0.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,17 +1,17 @@
-Zip file size: 1838264 bytes, number of entries: 359
+Zip file size: 1839177 bytes, number of entries: 359
 -rw-rw-r--  2.0 unx     1411 b- defN 23-Apr-09 06:29 slideflow/__init__.py
 -rw-rw-r--  2.0 unx     1662 b- defN 23-Apr-09 06:29 slideflow/_backend.py
--rw-rw-r--  2.0 unx      503 b- defN 23-Apr-17 19:25 slideflow/_version.py
--rw-rw-r--  2.0 unx   159745 b- defN 23-Apr-14 13:28 slideflow/dataset.py
--rw-rw-r--  2.0 unx     3733 b- defN 23-Apr-09 06:29 slideflow/errors.py
+-rw-rw-r--  2.0 unx      497 b- defN 23-Apr-21 22:20 slideflow/_version.py
+-rw-rw-r--  2.0 unx   159998 b- defN 23-Apr-21 22:19 slideflow/dataset.py
+-rw-rw-r--  2.0 unx     3733 b- defN 23-Apr-21 22:19 slideflow/errors.py
 -rw-rw-r--  2.0 unx    38304 b- defN 23-Apr-09 06:29 slideflow/heatmap.py
--rw-rw-r--  2.0 unx    25615 b- defN 23-Apr-09 06:29 slideflow/mosaic.py
--rw-rw-r--  2.0 unx   168044 b- defN 23-Apr-09 06:29 slideflow/project.py
--rw-rw-r--  2.0 unx    33320 b- defN 23-Apr-09 06:29 slideflow/project_utils.py
+-rw-rw-r--  2.0 unx    25615 b- defN 23-Apr-21 22:19 slideflow/mosaic.py
+-rw-rw-r--  2.0 unx   168044 b- defN 23-Apr-21 22:19 slideflow/project.py
+-rw-rw-r--  2.0 unx    33332 b- defN 23-Apr-19 02:21 slideflow/project_utils.py
 -rw-rw-r--  2.0 unx      978 b- defN 22-Jul-18 12:12 slideflow/sample_actions.py
 -rw-rw-r--  2.0 unx     1195 b- defN 23-Apr-09 06:29 slideflow/biscuit/__init__.py
 -rw-rw-r--  2.0 unx     4281 b- defN 23-Apr-09 06:29 slideflow/biscuit/delong.py
 -rw-rw-r--  2.0 unx      318 b- defN 23-Apr-09 06:29 slideflow/biscuit/errors.py
 -rw-rw-r--  2.0 unx    46729 b- defN 23-Apr-09 06:29 slideflow/biscuit/experiment.py
 -rw-rw-r--  2.0 unx     1260 b- defN 23-Apr-09 06:29 slideflow/biscuit/hp.py
 -rw-rw-r--  2.0 unx    21373 b- defN 23-Apr-09 06:29 slideflow/biscuit/threshold.py
@@ -150,51 +150,51 @@
 -rw-rw-r--  2.0 unx     9460 b- defN 23-Apr-05 13:33 slideflow/gan/stylegan3/stylegan3/viz/pickle_widget.py
 -rw-rw-r--  2.0 unx    24417 b- defN 22-Oct-30 14:59 slideflow/gan/stylegan3/stylegan3/viz/renderer.py
 -rw-rw-r--  2.0 unx     5573 b- defN 23-Apr-03 13:00 slideflow/gan/stylegan3/stylegan3/viz/stylemix_widget.py
 -rw-rw-r--  2.0 unx     5825 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/viz/thumb_widget.py
 -rw-rw-r--  2.0 unx     3845 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/viz/trunc_noise_widget.py
 -rw-rw-r--  2.0 unx    15662 b- defN 23-Apr-09 06:29 slideflow/grad/__init__.py
 -rw-rw-r--  2.0 unx     7376 b- defN 23-Feb-01 21:03 slideflow/grad/plot_utils.py
--rw-rw-r--  2.0 unx    11715 b- defN 23-Apr-09 06:29 slideflow/io/__init__.py
+-rw-rw-r--  2.0 unx    11715 b- defN 23-Apr-21 22:19 slideflow/io/__init__.py
 -rw-rw-r--  2.0 unx    10541 b- defN 22-Jul-18 12:12 slideflow/io/gaussian.py
 -rw-rw-r--  2.0 unx     9918 b- defN 23-Apr-09 06:29 slideflow/io/io_utils.py
--rw-rw-r--  2.0 unx    34511 b- defN 23-Apr-09 06:29 slideflow/io/tensorflow.py
--rw-rw-r--  2.0 unx    44556 b- defN 23-Apr-09 06:29 slideflow/io/torch.py
+-rw-rw-r--  2.0 unx    34511 b- defN 23-Apr-21 22:19 slideflow/io/tensorflow.py
+-rw-rw-r--  2.0 unx    45085 b- defN 23-Apr-21 22:19 slideflow/io/torch.py
 -rw-rw-r--  2.0 unx       68 b- defN 23-Mar-13 02:04 slideflow/io/preservedsite/__init__.py
 -rw-rw-r--  2.0 unx     8419 b- defN 23-Mar-13 02:04 slideflow/io/preservedsite/crossfolds.py
 -rw-rw-r--  2.0 unx      284 b- defN 23-Apr-09 06:29 slideflow/mil/__init__.py
 -rw-rw-r--  2.0 unx    14819 b- defN 23-Apr-09 06:29 slideflow/mil/_params.py
 -rw-rw-r--  2.0 unx     5160 b- defN 23-Apr-14 13:28 slideflow/mil/data.py
--rw-rw-r--  2.0 unx    15565 b- defN 23-Apr-17 18:21 slideflow/mil/eval.py
+-rw-rw-r--  2.0 unx    15565 b- defN 23-Apr-21 21:46 slideflow/mil/eval.py
 -rw-rw-r--  2.0 unx     3890 b- defN 23-Apr-09 06:29 slideflow/mil/clam/__init__.py
 -rw-rw-r--  2.0 unx     4334 b- defN 23-Apr-09 06:29 slideflow/mil/clam/create_attention.py
 -rw-rw-r--  2.0 unx     1131 b- defN 23-Apr-09 06:29 slideflow/mil/clam/datasets/__init__.py
 -rw-rw-r--  2.0 unx    14990 b- defN 23-Apr-09 06:29 slideflow/mil/clam/datasets/dataset_generic.py
 -rw-rw-r--  2.0 unx     5914 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/__init__.py
 -rw-rw-r--  2.0 unx    19782 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/core_utils.py
 -rw-rw-r--  2.0 unx     4150 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/eval_utils.py
 -rw-rw-r--  2.0 unx     1287 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/file_utils.py
 -rw-rw-r--  2.0 unx     3497 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/loss_utils.py
 -rw-rw-r--  2.0 unx      185 b- defN 23-Apr-09 06:29 slideflow/mil/models/__init__.py
 -rw-rw-r--  2.0 unx      378 b- defN 23-Apr-09 06:29 slideflow/mil/models/_utils.py
--rw-rw-r--  2.0 unx     3239 b- defN 23-Apr-09 06:29 slideflow/mil/models/att_mil.py
+-rw-rw-r--  2.0 unx     3239 b- defN 23-Apr-21 21:43 slideflow/mil/models/att_mil.py
 -rw-rw-r--  2.0 unx    12816 b- defN 23-Apr-09 06:29 slideflow/mil/models/clam.py
--rw-rw-r--  2.0 unx     3607 b- defN 23-Apr-09 06:29 slideflow/mil/models/mil_fc.py
+-rw-rw-r--  2.0 unx     3865 b- defN 23-Apr-18 22:07 slideflow/mil/models/mil_fc.py
 -rw-rw-r--  2.0 unx     4137 b- defN 23-Apr-09 06:29 slideflow/mil/models/transmil.py
 -rw-rw-r--  2.0 unx    15008 b- defN 23-Apr-12 04:31 slideflow/mil/train/__init__.py
 -rw-rw-r--  2.0 unx     8214 b- defN 23-Apr-12 04:31 slideflow/mil/train/_fastai.py
 -rw-rw-r--  2.0 unx     7795 b- defN 23-Apr-09 06:29 slideflow/mil/train/_legacy.py
 -rw-rw-r--  2.0 unx     7485 b- defN 23-Apr-09 06:29 slideflow/model/__init__.py
 -rw-rw-r--  2.0 unx     1879 b- defN 23-Mar-13 02:04 slideflow/model/adv_utils.py
--rw-rw-r--  2.0 unx    23588 b- defN 23-Apr-13 19:18 slideflow/model/base.py
--rw-rw-r--  2.0 unx    55263 b- defN 23-Apr-17 18:21 slideflow/model/features.py
--rw-rw-r--  2.0 unx   110908 b- defN 23-Apr-14 13:28 slideflow/model/tensorflow.py
+-rw-rw-r--  2.0 unx    23588 b- defN 23-Apr-19 02:19 slideflow/model/base.py
+-rw-rw-r--  2.0 unx    55334 b- defN 23-Apr-21 22:19 slideflow/model/features.py
+-rw-rw-r--  2.0 unx   110908 b- defN 23-Apr-21 22:19 slideflow/model/tensorflow.py
 -rw-rw-r--  2.0 unx    22547 b- defN 23-Apr-03 05:27 slideflow/model/tensorflow_utils.py
--rw-rw-r--  2.0 unx   103179 b- defN 23-Apr-14 13:28 slideflow/model/torch.py
--rw-rw-r--  2.0 unx    16945 b- defN 23-Apr-09 06:29 slideflow/model/torch_utils.py
+-rw-rw-r--  2.0 unx   103271 b- defN 23-Apr-21 22:19 slideflow/model/torch.py
+-rw-rw-r--  2.0 unx    17005 b- defN 23-Apr-19 02:23 slideflow/model/torch_utils.py
 -rw-rw-r--  2.0 unx      394 b- defN 23-Apr-09 06:29 slideflow/model/extractors/__init__.py
 -rw-rw-r--  2.0 unx     3708 b- defN 23-Apr-09 06:29 slideflow/model/extractors/_factory.py
 -rw-rw-r--  2.0 unx     4544 b- defN 23-Apr-09 06:29 slideflow/model/extractors/_factory_tensorflow.py
 -rw-rw-r--  2.0 unx     5711 b- defN 23-Apr-09 06:29 slideflow/model/extractors/_factory_torch.py
 -rw-rw-r--  2.0 unx     1570 b- defN 23-Apr-09 06:29 slideflow/model/extractors/_registry.py
 -rw-rw-r--  2.0 unx     2646 b- defN 23-Apr-09 06:29 slideflow/model/extractors/_slide.py
 -rw-rw-r--  2.0 unx    25681 b- defN 23-Apr-09 06:29 slideflow/model/extractors/ctranspath.py
@@ -207,49 +207,49 @@
 -rw-rw-r--  2.0 unx    15314 b- defN 23-Apr-09 06:29 slideflow/norm/utils.py
 -rw-rw-r--  2.0 unx     7836 b- defN 23-Apr-09 06:29 slideflow/norm/vahadane.py
 -rw-rw-r--  2.0 unx    11998 b- defN 23-Apr-09 06:29 slideflow/norm/tensorflow/__init__.py
 -rw-rw-r--  2.0 unx     5879 b- defN 22-Jul-15 00:02 slideflow/norm/tensorflow/color.py
 -rw-rw-r--  2.0 unx    20759 b- defN 23-Apr-09 06:29 slideflow/norm/tensorflow/macenko.py
 -rw-rw-r--  2.0 unx    26169 b- defN 23-Apr-09 06:29 slideflow/norm/tensorflow/reinhard.py
 -rw-rw-r--  2.0 unx     1685 b- defN 23-Apr-09 06:29 slideflow/norm/tensorflow/utils.py
--rw-rw-r--  2.0 unx    11313 b- defN 23-Apr-09 06:29 slideflow/norm/torch/__init__.py
+-rw-rw-r--  2.0 unx    12095 b- defN 23-Apr-19 02:24 slideflow/norm/torch/__init__.py
 -rw-rw-r--  2.0 unx     7826 b- defN 22-Aug-06 13:13 slideflow/norm/torch/color.py
--rw-rw-r--  2.0 unx    14869 b- defN 23-Apr-09 06:29 slideflow/norm/torch/macenko.py
--rw-rw-r--  2.0 unx    24997 b- defN 23-Apr-09 06:29 slideflow/norm/torch/reinhard.py
+-rw-rw-r--  2.0 unx    15296 b- defN 23-Apr-21 22:19 slideflow/norm/torch/macenko.py
+-rw-rw-r--  2.0 unx    24997 b- defN 23-Apr-19 00:55 slideflow/norm/torch/reinhard.py
 -rw-rw-r--  2.0 unx     1673 b- defN 23-Apr-09 06:29 slideflow/norm/torch/utils.py
 -rw-rw-r--  2.0 unx      458 b- defN 23-Mar-13 02:04 slideflow/simclr/__init__.py
 -rw-rw-r--  2.0 unx        6 b- defN 23-Feb-07 02:07 slideflow/simclr/simclr/__init__.py
--rw-rw-r--  2.0 unx    20637 b- defN 23-Feb-15 14:12 slideflow/simclr/simclr/tf2/__init__.py
+-rw-rw-r--  2.0 unx    21046 b- defN 23-Apr-21 22:18 slideflow/simclr/simclr/tf2/__init__.py
 -rw-rw-r--  2.0 unx    10701 b- defN 23-Apr-17 17:52 slideflow/simclr/simclr/tf2/data.py
 -rw-rw-r--  2.0 unx    18550 b- defN 23-Feb-03 23:00 slideflow/simclr/simclr/tf2/data_util.py
 -rw-rw-r--  2.0 unx     6505 b- defN 23-Apr-17 18:03 slideflow/simclr/simclr/tf2/lars_optimizer.py
 -rw-rw-r--  2.0 unx     2997 b- defN 23-Jan-23 19:16 slideflow/simclr/simclr/tf2/metrics.py
 -rw-rw-r--  2.0 unx    12256 b- defN 23-Feb-03 23:00 slideflow/simclr/simclr/tf2/model.py
 -rw-rw-r--  2.0 unx     4983 b- defN 23-Jan-23 19:16 slideflow/simclr/simclr/tf2/objective.py
 -rw-rw-r--  2.0 unx    28397 b- defN 23-Feb-03 23:00 slideflow/simclr/simclr/tf2/resnet.py
 -rw-rw-r--  2.0 unx     6524 b- defN 23-Feb-03 23:00 slideflow/simclr/simclr/tf2/run.py
 -rw-rw-r--  2.0 unx     9517 b- defN 23-Feb-06 18:24 slideflow/simclr/simclr/tf2/utils.py
--rw-rw-r--  2.0 unx   114717 b- defN 23-Apr-17 19:23 slideflow/slide/__init__.py
--rw-rw-r--  2.0 unx    19045 b- defN 23-Apr-09 06:29 slideflow/slide/report.py
+-rw-rw-r--  2.0 unx   114953 b- defN 23-Apr-21 22:19 slideflow/slide/__init__.py
+-rw-rw-r--  2.0 unx    19076 b- defN 23-Apr-21 17:48 slideflow/slide/report.py
 -rw-rw-r--  2.0 unx    30934 b- defN 23-Apr-09 06:29 slideflow/slide/slideflow-logo-name-small.jpg
 -rw-rw-r--  2.0 unx     5516 b- defN 23-Apr-10 16:45 slideflow/slide/utils.py
 -rw-rw-r--  2.0 unx      708 b- defN 23-Mar-01 23:20 slideflow/slide/backends/__init__.py
 -rw-rw-r--  2.0 unx    14008 b- defN 23-Apr-09 06:29 slideflow/slide/backends/cucim.py
--rw-rw-r--  2.0 unx    22391 b- defN 23-Apr-13 22:48 slideflow/slide/backends/vips.py
--rw-rw-r--  2.0 unx      117 b- defN 23-Mar-13 02:04 slideflow/slide/qc/__init__.py
+-rw-rw-r--  2.0 unx    22391 b- defN 23-Apr-21 22:19 slideflow/slide/backends/vips.py
+-rw-rw-r--  2.0 unx      117 b- defN 23-Apr-21 22:19 slideflow/slide/qc/__init__.py
 -rw-rw-r--  2.0 unx     1010 b- defN 22-Nov-29 16:52 slideflow/slide/qc/deepfocus_qc.py
--rw-rw-r--  2.0 unx     4304 b- defN 23-Apr-14 15:00 slideflow/slide/qc/gaussian.py
--rw-rw-r--  2.0 unx     5232 b- defN 23-Apr-09 06:29 slideflow/slide/qc/otsu.py
+-rw-rw-r--  2.0 unx     4304 b- defN 23-Apr-21 22:19 slideflow/slide/qc/gaussian.py
+-rw-rw-r--  2.0 unx     5232 b- defN 23-Apr-21 22:19 slideflow/slide/qc/otsu.py
 -rw-rw-r--  2.0 unx     3028 b- defN 23-Mar-13 02:04 slideflow/slide/qc/saver.py
--rw-rw-r--  2.0 unx     4676 b- defN 23-Mar-13 02:04 slideflow/slide/qc/strided_dl.py
+-rw-rw-r--  2.0 unx     4676 b- defN 23-Apr-21 22:19 slideflow/slide/qc/strided_dl.py
 -rw-rw-r--  2.0 unx      390 b- defN 23-Feb-01 21:02 slideflow/stats/__init__.py
 -rw-rw-r--  2.0 unx     4293 b- defN 23-Apr-09 06:29 slideflow/stats/delong.py
 -rw-rw-r--  2.0 unx    35785 b- defN 23-Apr-09 06:29 slideflow/stats/metrics.py
 -rw-rw-r--  2.0 unx     5757 b- defN 23-Apr-09 06:29 slideflow/stats/plot.py
--rw-rw-r--  2.0 unx    43101 b- defN 23-Apr-09 06:29 slideflow/stats/slidemap.py
+-rw-rw-r--  2.0 unx    43101 b- defN 23-Apr-21 22:19 slideflow/stats/slidemap.py
 -rw-rw-r--  2.0 unx     3261 b- defN 23-Apr-09 06:29 slideflow/stats/stats_utils.py
 -rw-rw-r--  2.0 unx    77576 b- defN 23-Apr-09 06:29 slideflow/studio/__init__.py
 -rw-rw-r--  2.0 unx     2187 b- defN 23-Apr-09 06:29 slideflow/studio/__main__.py
 -rw-rw-r--  2.0 unx    19662 b- defN 23-Apr-09 06:29 slideflow/studio/_renderer.py
 -rw-rw-r--  2.0 unx     3686 b- defN 23-Apr-09 06:29 slideflow/studio/utils.py
 -rw-rw-r--  2.0 unx        8 b- defN 23-Apr-09 06:29 slideflow/studio/gui/__init__.py
 -rw-rw-r--  2.0 unx    14387 b- defN 23-Apr-09 06:29 slideflow/studio/gui/_glfw.py
@@ -305,57 +305,57 @@
 -rw-rw-r--  2.0 unx   190776 b- defN 23-Apr-09 06:29 slideflow/studio/gui/fonts/DroidSans.ttf
 -rw-rw-r--  2.0 unx     7568 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/error.png
 -rw-rw-r--  2.0 unx     7286 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/info.png
 -rw-rw-r--  2.0 unx   125763 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/logo.png
 -rw-rw-r--  2.0 unx     7181 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/success.png
 -rw-rw-r--  2.0 unx     6817 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/warn.png
 -rw-rw-r--  2.0 unx      107 b- defN 23-Apr-09 06:29 slideflow/studio/gui/viewer/__init__.py
--rw-rw-r--  2.0 unx     7874 b- defN 23-Apr-09 06:29 slideflow/studio/gui/viewer/_mosaic.py
--rw-rw-r--  2.0 unx    24832 b- defN 23-Apr-17 19:21 slideflow/studio/gui/viewer/_slide.py
+-rw-rw-r--  2.0 unx     7874 b- defN 23-Apr-21 17:43 slideflow/studio/gui/viewer/_mosaic.py
+-rw-rw-r--  2.0 unx    26041 b- defN 23-Apr-18 20:39 slideflow/studio/gui/viewer/_slide.py
 -rw-rw-r--  2.0 unx    14452 b- defN 23-Apr-09 06:29 slideflow/studio/gui/viewer/_viewer.py
 -rw-rw-r--  2.0 unx      439 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/__init__.py
 -rw-rw-r--  2.0 unx      735 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/_utils.py
 -rw-rw-r--  2.0 unx     4264 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/capture.py
 -rw-rw-r--  2.0 unx     5412 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/extensions.py
 -rw-rw-r--  2.0 unx    16890 b- defN 23-Apr-14 13:28 slideflow/studio/widgets/heatmap.py
 -rw-rw-r--  2.0 unx     3842 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/layer_umap.py
 -rw-rw-r--  2.0 unx    24474 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/model.py
--rw-rw-r--  2.0 unx    14307 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/mosaic.py
+-rw-rw-r--  2.0 unx    14307 b- defN 23-Apr-21 22:19 slideflow/studio/widgets/mosaic.py
 -rw-rw-r--  2.0 unx     2597 b- defN 23-Mar-19 15:05 slideflow/studio/widgets/mosaic_experimental.py
 -rw-rw-r--  2.0 unx     4662 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/performance.py
 -rw-rw-r--  2.0 unx     6437 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/picam.py
 -rw-rw-r--  2.0 unx     7726 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/project.py
 -rw-rw-r--  2.0 unx     5983 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/seed_map.py
 -rw-rw-r--  2.0 unx    19875 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/segment.py
 -rw-rw-r--  2.0 unx     2071 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/settings.py
 -rw-rw-r--  2.0 unx    31018 b- defN 23-Apr-14 13:28 slideflow/studio/widgets/slide.py
 -rw-rw-r--  2.0 unx    16402 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/stylegan.py
--rw-rw-r--  2.0 unx    32309 b- defN 23-Apr-09 06:29 slideflow/test/__init__.py
+-rw-rw-r--  2.0 unx    32309 b- defN 23-Apr-21 18:07 slideflow/test/__init__.py
 -rw-rw-r--  2.0 unx    12446 b- defN 23-Feb-01 21:02 slideflow/test/dataset_test.py
 -rw-rw-r--  2.0 unx     9560 b- defN 23-Mar-26 19:33 slideflow/test/functional.py
 -rw-rw-r--  2.0 unx     8052 b- defN 23-Apr-09 06:29 slideflow/test/model_test.py
 -rw-rw-r--  2.0 unx    12098 b- defN 23-Mar-01 23:20 slideflow/test/norm_test.py
 -rw-rw-r--  2.0 unx     2909 b- defN 23-Apr-09 06:29 slideflow/test/slide_test.py
 -rw-rw-r--  2.0 unx    11481 b- defN 23-Mar-13 02:04 slideflow/test/stats_test.py
 -rw-rw-r--  2.0 unx    11752 b- defN 23-Apr-09 06:29 slideflow/test/utils.py
 -rw-rw-r--  2.0 unx      891 b- defN 23-Mar-13 02:04 slideflow/tfrecord/__init__.py
 -rw-rw-r--  2.0 unx     2905 b- defN 22-Dec-02 04:52 slideflow/tfrecord/iterator_utils.py
--rw-rw-r--  2.0 unx    15505 b- defN 23-Jan-29 22:26 slideflow/tfrecord/reader.py
+-rw-rw-r--  2.0 unx    15505 b- defN 23-Apr-21 04:57 slideflow/tfrecord/reader.py
 -rw-rw-r--  2.0 unx     5637 b- defN 22-Jul-18 12:12 slideflow/tfrecord/writer.py
 -rw-rw-r--  2.0 unx      179 b- defN 22-Jul-18 12:12 slideflow/tfrecord/tools/__init__.py
 -rw-rw-r--  2.0 unx      310 b- defN 22-Jul-18 12:12 slideflow/tfrecord/torch/__init__.py
 -rw-rw-r--  2.0 unx     7857 b- defN 22-Jul-18 12:12 slideflow/tfrecord/torch/dataset.py
--rw-rw-r--  2.0 unx    41104 b- defN 23-Apr-13 22:11 slideflow/util/__init__.py
+-rw-rw-r--  2.0 unx    41152 b- defN 23-Apr-21 22:19 slideflow/util/__init__.py
 -rw-rw-r--  2.0 unx      738 b- defN 22-Jul-15 00:02 slideflow/util/colors.py
 -rw-rw-r--  2.0 unx    17912 b- defN 22-Jul-18 12:12 slideflow/util/example_pb2.py
 -rw-rw-r--  2.0 unx     4468 b- defN 23-Feb-01 21:02 slideflow/util/log_utils.py
 -rw-rw-r--  2.0 unx     4381 b- defN 22-Jul-18 12:12 slideflow/util/neptune_utils.py
 -rw-rw-r--  2.0 unx    20061 b- defN 23-Apr-09 06:29 slideflow/util/smac_utils.py
--rw-rw-r--  2.0 unx     8046 b- defN 23-Apr-17 18:21 slideflow/util/tfrecord2idx.py
--rwxrwxr-x  2.0 unx    14085 b- defN 23-Apr-17 19:25 slideflow-2.0.2.post1.data/scripts/slideflow-studio
--rwxrwxr-x  2.0 unx    14085 b- defN 23-Apr-10 13:18 slideflow-2.0.2.post1.data/scripts/slideflow-studio.py
--rw-rw-r--  2.0 unx    35149 b- defN 23-Apr-17 19:25 slideflow-2.0.2.post1.dist-info/LICENSE
--rw-rw-r--  2.0 unx    13004 b- defN 23-Apr-17 19:25 slideflow-2.0.2.post1.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-17 19:25 slideflow-2.0.2.post1.dist-info/WHEEL
--rw-rw-r--  2.0 unx       10 b- defN 23-Apr-17 19:25 slideflow-2.0.2.post1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    35690 b- defN 23-Apr-17 19:25 slideflow-2.0.2.post1.dist-info/RECORD
-359 files, 4768869 bytes uncompressed, 1780544 bytes compressed:  62.7%
+-rw-rw-r--  2.0 unx     8064 b- defN 23-Apr-21 22:19 slideflow/util/tfrecord2idx.py
+-rwxrwxr-x  2.0 unx    14085 b- defN 23-Apr-21 22:20 slideflow-2.0.3.data/scripts/slideflow-studio
+-rwxrwxr-x  2.0 unx    14085 b- defN 23-Apr-10 13:18 slideflow-2.0.3.data/scripts/slideflow-studio.py
+-rw-rw-r--  2.0 unx    35149 b- defN 23-Apr-21 22:20 slideflow-2.0.3.dist-info/LICENSE
+-rw-rw-r--  2.0 unx    13019 b- defN 23-Apr-21 22:20 slideflow-2.0.3.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Apr-21 22:20 slideflow-2.0.3.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       10 b- defN 23-Apr-21 22:20 slideflow-2.0.3.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    35648 b- defN 23-Apr-21 22:20 slideflow-2.0.3.dist-info/RECORD
+359 files, 4773271 bytes uncompressed, 1781541 bytes compressed:  62.7%
```

## zipnote {}

```diff
@@ -1050,29 +1050,29 @@
 
 Filename: slideflow/util/smac_utils.py
 Comment: 
 
 Filename: slideflow/util/tfrecord2idx.py
 Comment: 
 
-Filename: slideflow-2.0.2.post1.data/scripts/slideflow-studio
+Filename: slideflow-2.0.3.data/scripts/slideflow-studio
 Comment: 
 
-Filename: slideflow-2.0.2.post1.data/scripts/slideflow-studio.py
+Filename: slideflow-2.0.3.data/scripts/slideflow-studio.py
 Comment: 
 
-Filename: slideflow-2.0.2.post1.dist-info/LICENSE
+Filename: slideflow-2.0.3.dist-info/LICENSE
 Comment: 
 
-Filename: slideflow-2.0.2.post1.dist-info/METADATA
+Filename: slideflow-2.0.3.dist-info/METADATA
 Comment: 
 
-Filename: slideflow-2.0.2.post1.dist-info/WHEEL
+Filename: slideflow-2.0.3.dist-info/WHEEL
 Comment: 
 
-Filename: slideflow-2.0.2.post1.dist-info/top_level.txt
+Filename: slideflow-2.0.3.dist-info/top_level.txt
 Comment: 
 
-Filename: slideflow-2.0.2.post1.dist-info/RECORD
+Filename: slideflow-2.0.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## slideflow/_version.py

```diff
@@ -4,18 +4,18 @@
 # unpacked source archive. Distribution tarballs contain a pre-generated copy
 # of this file.
 
 import json
 
 version_json = '''
 {
- "date": "2023-04-17T14:24:21-0500",
+ "date": "2023-04-21T17:19:26-0500",
  "dirty": false,
  "error": null,
- "full-revisionid": "5ea8c7ab2f30763a20c1cb069e6cd60dc471c5ef",
- "version": "2.0.2.post1"
+ "full-revisionid": "b64be73fa730ab23f9434cab6b857c9c3ec0c8b0",
+ "version": "2.0.3"
 }
 '''  # END VERSION_JSON
 
 
 def get_versions():
     return json.loads(version_json)
```

## slideflow/dataset.py

```diff
@@ -901,28 +901,37 @@
             total_prob = sum([cat_prob[tfr_cats[tfr]] for tfr in tfrecords])
             ret.prob_weights = {
                 tfr: cat_prob[tfr_cats[tfr]]/total_prob for tfr in tfrecords
             }
         return ret
 
     def build_index(self, force: bool = True) -> None:
-        """Build index files for TFRecords. Required for PyTorch.
+        """Build index files for TFRecords.
 
         Args:
             force (bool): Force re-build existing indices.
 
         Returns:
             None
         """
+        if force:
+            missing_index = self.tfrecords()
+        else:
+            missing_index = [
+                tfr for tfr in self.tfrecords()
+                if not tfrecord2idx.find_index(tfr)
+            ]
+            if not missing_index:
+                return
+        index_fn = partial(_create_index, force=force)
         pool = mp.Pool(
             os.cpu_count(),
             initializer=sf.util.set_ignore_sigint
         )
-        index_fn = partial(_create_index, force=force)
-        for _ in track(pool.imap_unordered(index_fn, self.tfrecords()),
+        for _ in track(pool.imap_unordered(index_fn, missing_index),
                        description='Creating index files...',
                        total=len(self.tfrecords()),
                        transient=True):
             pass
         pool.close()
 
     def cell_segmentation(
```

## slideflow/project_utils.py

```diff
@@ -946,17 +946,17 @@
 class BreastER(_ProjectConfig):
     config_url = 'https://raw.githubusercontent.com/jamesdolezal/slideflow/1.4.3/datasets/breast_er/breast_er.json'
     config_md5 = '6732f7e2473e2d58bc88a7aca1f0e770'
     labels_url = 'https://raw.githubusercontent.com/jamesdolezal/slideflow/1.4.3/datasets/breast_er/breast_labels.csv'
     labels_md5 = 'e25028e87760749973ceea691e6d63d7'
 
 class ThyroidBRS(_ProjectConfig):
-    config_url = 'https://raw.githubusercontent.com/jamesdolezal/slideflow/dev/datasets/thyroid_brs/thyroid_brs.json'
+    config_url = 'https://raw.githubusercontent.com/jamesdolezal/slideflow/master/datasets/thyroid_brs/thyroid_brs.json'
     config_md5 = 'c4fbe83766db8f637780f7881cb1045e'
-    labels_url = 'https://raw.githubusercontent.com/jamesdolezal/slideflow/dev/datasets/thyroid_brs/thyroid_labels.csv'
+    labels_url = 'https://raw.githubusercontent.com/jamesdolezal/slideflow/master/datasets/thyroid_brs/thyroid_labels.csv'
     labels_md5 = 'c04f2569dc3a914241fae0d0b644a327'
 
 class LungAdenoSquam(_ProjectConfig):
-    config_url = 'https://raw.githubusercontent.com/jamesdolezal/slideflow/dev/datasets/lung_adeno_squam/lung_adeno_squam.json'
+    config_url = 'https://raw.githubusercontent.com/jamesdolezal/slideflow/master/datasets/lung_adeno_squam/lung_adeno_squam.json'
     config_md5 = '9239d18b66e054132700c08831560669'
-    labels_url = 'https://raw.githubusercontent.com/jamesdolezal/slideflow/dev/datasets/lung_adeno_squam/lung_labels.csv'
+    labels_url = 'https://raw.githubusercontent.com/jamesdolezal/slideflow/master/datasets/lung_adeno_squam/lung_labels.csv'
     labels_md5 = '6619d520d707e211b22b477996bcfdcd'
```

## slideflow/io/torch.py

```diff
@@ -2,14 +2,15 @@
 import os
 import random
 import threading
 import numpy as np
 import pandas as pd
 import torchvision
 import torch
+import math
 from torchvision import transforms
 from os import listdir
 from os.path import dirname, exists, isfile, join
 from queue import Queue
 from typing import (TYPE_CHECKING, Any, Callable, Dict, Iterable, List,
                     Optional, Tuple, Union)
 
@@ -64,15 +65,16 @@
         device: Optional[torch.device] = None,
         max_size: int = 0,
         from_wsi: bool = False,
         tile_um: Optional[int] = None,
         rois: Optional[List[str]] = None,
         roi_method: str = 'auto',
         pool: Optional[Any] = None,
-        transform: Optional[Any] = None
+        transform: Optional[Any] = None,
+        **interleave_kwargs
     ) -> None:
         """Pytorch IterableDataset that interleaves tfrecords with
         :func:`slideflow.io.torch.interleave`.
 
         Args:
             tfrecords (list(str)): Path to tfrecord files to interleave.
             img_size (int): Image width in pixels.
@@ -141,14 +143,15 @@
         self.device = device
         self.from_wsi = from_wsi
         self.tile_um = tile_um
         self.rois = rois
         self.roi_method = roi_method
         self.pool = pool
         self.transform = transform
+        self.interleave_kwargs = interleave_kwargs
 
         # Values for random label generation, for GAN
         if labels is not None:
             if self.onehot:
                 _all_labels_raw = np.array(list(labels.values()))
                 _unique_raw = np.unique(_all_labels_raw)
                 max_label = np.max(_unique_raw)
@@ -282,15 +285,16 @@
             device=self.device,
             tile_px=self.img_size,
             from_wsi=self.from_wsi,
             tile_um=self.tile_um,
             rois=self.rois,
             roi_method=self.roi_method,
             pool=self.pool,
-            transform=self.transform
+            transform=self.transform,
+            **self.interleave_kwargs
         )
         self.close = queue_retriever.close
         try:
             for record in queue_retriever:
                 yield self._parser(*record)
         # Closes open files if iterator terminated early
         except GeneratorExit as e:
@@ -1004,14 +1008,15 @@
             self.sampler = sampler
             self.closed = False
             self.raw_q = Queue(num_threads)
             self.proc_q = Queue(num_threads)
             self.n_threads = num_threads
             self.n_closed = 0
             self.il_closed = False
+            self._close_complete = False
 
             def interleaver():
                 msg = []
                 while not self.closed:
                     try:
                         record = next(sampler_iter)
                         msg += [record]
@@ -1057,28 +1062,34 @@
                     self.n_closed += 1
                     if self.n_closed == self.n_threads:
                         break
                 else:
                     for item in record:
                         yield item
 
+        def __del__(self):
+            self.close()
+
         def close(self):
+            if self._close_complete:
+                return
             log.debug("Closing QueueRetriever")
             self.closed = True
 
             # Clear out the queue
             while self.n_closed < self.n_threads:
                 record = self.proc_q.get()
                 if record is None:
                     self.n_closed += 1
 
             if from_wsi and should_close:
                 pool.close()
             else:
                 self.sampler.close()
+            self._close_complete = True
 
     return QueueRetriever(random_sampler, num_threads)
 
 
 def interleave_dataloader(
     tfrecords: List[str],
     img_size: int,
@@ -1163,14 +1174,17 @@
                          "num_workers > 0")
 
     if num_workers is None and os.cpu_count():
         num_workers = os.cpu_count() // 4  # type: ignore
     elif num_workers is None:
         num_workers = 8
     log.debug(f"Using num_workers={num_workers}")
+    if 'num_threads' not in kwargs and os.cpu_count():
+        kwargs['num_threads'] = int(math.ceil(os.cpu_count() / max(num_workers, 1)))
+        log.debug(f"Threads per worker={kwargs['num_threads']}")
 
     iterator = InterleaveIterator(
         tfrecords=tfrecords,
         img_size=img_size,
         use_labels=(labels is not None),
         num_replicas=num_replicas,
         labels=labels,
```

## slideflow/mil/eval.py

```diff
@@ -393,15 +393,15 @@
         )
         attention = False
     for bag in bags:
         loaded = torch.load(bag).to(device)
         loaded = torch.unsqueeze(loaded, dim=0)
         with torch.no_grad():
             if use_lens:
-                lens = torch.from_numpy(np.array([loaded.shape[0]])).to(device)
+                lens = torch.from_numpy(np.array([loaded.shape[1]])).to(device)
                 model_args = (loaded, lens)
             else:
                 model_args = (loaded,)
             model_out = model(*model_args)
             if attention:
                 att = torch.squeeze(model.calculate_attention(*model_args))
                 if len(att.shape) == 2:
```

## slideflow/mil/models/mil_fc.py

```diff
@@ -67,15 +67,20 @@
         size: Union[str, List[int]] = "small",
         dropout: bool = False,
         n_classes: int = 2,
         top_k: int = 1,
         gate: bool = True,
     ):
         super().__init__()
-        assert n_classes > 2
+        if not n_classes > 2:
+            raise ValueError(
+                "The 'MIL_fc_mc' model is a multi-categorical model that "
+                "requires more than two outcome categories. For binary outcomes "
+                "with only 2 categories, use 'MIL_fc'."
+            )
 
         self.size = self.sizes[size] if isinstance(size, str) else size
 
         fc = [nn.Linear(self.size[0], self.size[1]), nn.ReLU()]
         if dropout:
             fc.append(nn.Dropout(0.25))
         self.fc = nn.Sequential(*fc)
```

## slideflow/model/features.py

```diff
@@ -238,15 +238,15 @@
         dfs = []
         for f, ftrs in enumerate(args):
             log.debug(f"Creating dataframe {f} from features...")
             dfs.append(ftrs.to_df())
         if not all([len(df) == len(dfs[0]) for df in dfs]):
             raise ValueError(
                 "Unable to concatenate DatasetFeatures of different lengths "
-                f"(got: {', '.join([len(_df) for _df in dfs])})"
+                f"(got: {', '.join([str(len(_df)) for _df in dfs])})"
             )
         log.debug(f"Created {len(dfs)} dataframes")
         for i in range(len(dfs)):
             log.debug(f"Mapping tuples for df {i}")
             dfs[i]['locations'] = dfs[i]['locations'].map(tuple)
         for i in range(1, len(dfs)):
             log.debug(f"Merging dataframe {i}")
@@ -1407,9 +1407,11 @@
         with sf.util.cleanup_progress(pb):
             for batch_img, _, batch_slides, batch_loc_x, batch_loc_y in dataset:
                 model_output = self._calculate_feature_batch(batch_img)
                 q.put((model_output, batch_slides, (batch_loc_x, batch_loc_y)))
                 pb.advance(task, self.batch_size)
         q.put((None, None, None))
         batch_proc_thread.join()
+        if hasattr(dataset, 'close'):
+            dataset.close()
 
         return activations, predictions, locations, uncertainty
```

## slideflow/model/torch.py

```diff
@@ -270,17 +270,14 @@
             'SmoothL1': torch.nn.SmoothL1Loss,
             'CosineEmbedding': torch.nn.CosineEmbeddingLoss,
         }
         super().__init__(loss=loss, **kwargs)
         assert self.model in self.ModelDict.keys() or self.model.startswith('timm_')
         assert self.optimizer in self.OptDict.keys()
         assert self.loss in self.AllLossDict
-        if isinstance(self.augment, str) and 'b' in self.augment:
-            log.warn('Gaussian blur not yet optimized in PyTorch backend; '
-                     'image pre-processing may be slow.')
         if self.model == 'inception':
             log.warn("Model 'inception' has an auxillary classifier, which "
                      "is currently ignored during training. Auxillary "
                      "classifier loss will be included during training "
                      "starting in version 1.3")
 
 
@@ -1047,14 +1044,15 @@
         running_val_loss = 0
         num_val = 0
         running_val_correct = self._empty_corrects()
 
         for _ in range(self.validation_steps):
             val_img, val_label, slides, *_ = next(self.mid_train_val_dts)  # type:ignore
             val_img = val_img.to(self.device)
+            val_img = val_img.to(memory_format=torch.channels_last)
 
             with torch.no_grad():
                 _mp = self.mixed_precision
                 _ns = no_scope()
                 with torch.cuda.amp.autocast() if _mp else _ns:  # type: ignore
 
                     # GPU normalization, if specified.
@@ -1263,14 +1261,15 @@
         else:
             log.debug('Validation during training: None')
 
     def _training_step(self, pb: Progress) -> None:
         assert self.model is not None
         images, labels, slides = next(self.dataloaders['train'])
         images = images.to(self.device, non_blocking=True)
+        images = images.to(memory_format=torch.channels_last)
         labels = self._labels_to_device(labels, self.device)
         self.optimizer.zero_grad()
         with torch.set_grad_enabled(True):
             _mp = self.mixed_precision
             _ns = no_scope()
             with torch.cuda.amp.autocast() if _mp else _ns:  # type: ignore
 
@@ -2199,15 +2198,16 @@
 
     def _predict(self, inp: Tensor, no_grad: bool = True) -> List[Tensor]:
         """Return activations for a single batch of images."""
         assert torch.is_floating_point(inp), "Input tensor must be float"
         _mp = self.mixed_precision
         with torch.cuda.amp.autocast() if _mp else no_scope():  # type: ignore
             with torch.no_grad() if no_grad else no_scope():
-                logits = self._model(inp.to(self.device))
+                inp = inp.to(self.device).to(memory_format=torch.channels_last)
+                logits = self._model(inp)
                 if isinstance(logits, (tuple, list)) and self.apply_softmax:
                     logits = [softmax(l, dim=1) for l in logits]
                 elif self.apply_softmax:
                     logits = softmax(logits, dim=1)
 
         layer_activations = []
         if self.layers:
@@ -2382,15 +2382,17 @@
 
         out_pred_drop = [[] for _ in range(self.num_outputs)]
         if self.layers:
             out_act_drop = [[] for _ in range(len(self.layers))]
         for _ in range(30):
             with torch.cuda.amp.autocast() if _mp else no_scope():  # type: ignore
                 with torch.no_grad() if no_grad else no_scope():
-                    logits = self._model(inp.to(self.device))
+                    inp = inp.to(self.device)
+                    inp = inp.to(memory_format=torch.channels_last)
+                    logits = self._model(inp)
                     if isinstance(logits, (tuple, list)) and self.apply_softmax:
                         logits = [softmax(l, dim=1) for l in logits]
                     elif self.apply_softmax:
                         logits = softmax(logits, dim=1)
                     for n in range(self.num_outputs):
                         out_pred_drop[n] += [
                             (logits[n] if self.num_outputs > 1 else logits)
```

## slideflow/model/torch_utils.py

```diff
@@ -279,14 +279,15 @@
             else:
                 img, yt, slide = batch
 
             if verbosity != 'silent':
                 pb.advance(task, img.shape[0])
 
             img = img.to(device, non_blocking=True)
+            img = img.to(memory_format=torch.channels_last)
             with torch.cuda.amp.autocast():
                 with torch.no_grad():
                     # GPU normalization
                     if torch_args is not None and torch_args.normalizer:
                         img = torch_args.normalizer.preprocess(img)
 
                     # Slide-level features
```

## slideflow/norm/torch/__init__.py

```diff
@@ -25,14 +25,15 @@
         'macenko_fast': macenko.MacenkoFastNormalizer
     }
 
     def __init__(
         self,
         method: str,
         device: Optional[str] = None,
+        batch_size: int = 32,
         **kwargs
     ) -> None:
         """PyTorch-native H&E Stain normalizer.
 
         The stain normalizer supports numpy images, PNG or JPG strings,
         Tensorflow tensors, and PyTorch tensors. The default `.transform()`
         method will attempt to preserve the original image type while minimizing
@@ -40,14 +41,21 @@
 
         Alternatively, you can manually specify the image conversion type
         by using the appropriate function. For example, to convert a JPEG
         image to a normalized numpy RGB image, use `.jpeg_to_rgb()`.
 
         Args:
             method (str): Normalization method to use.
+            device (str): Device (cpu, gpu) on which normalization should
+                be performed. Defaults to the preferred method for each
+                normalizer.
+            batch_size (int): Maximum batch size to use when performing
+                stain normalization. Will split larger batches into chunks.
+                Helps avoid "input tensor is too large" issues with
+                torch.quantile(). Defaults to 32.
 
         Keyword args:
             stain_matrix_target (np.ndarray, optional): Set the stain matrix
                 target for the normalizer. May raise an error if the normalizer
                 does not have a stain_matrix_target fit attribute.
             target_concentrations (np.ndarray, optional): Set the target
                 concentrations for the normalizer. May raise an error if the
@@ -64,14 +72,15 @@
 
         Examples
             Please see :class:`slideflow.norm.StainNormalizer` for examples.
         """
 
         super().__init__(method, **kwargs)
         self._device = device
+        self.batch_size = batch_size
 
     @property
     def device(self) -> str:
         """Device (e.g. cpu, gpu) on which normalization should be performed.
 
         Returns:
             str: Device that will be used.
@@ -216,15 +225,24 @@
                 auto-converted and permuted back after normalization.
 
         Returns:
             torch.Tensor:   Image, uint8.
 
         """
         from slideflow.io.torch import cwh_to_whc, whc_to_cwh, is_cwh
-        if is_cwh(inp):
+
+        if inp.ndim == 4 and inp.shape[0] > self.batch_size:
+            return torch.cat(
+                [
+                    self._torch_transform(t)
+                    for t in torch.split(inp, self.batch_size)
+                ],
+                dim=0
+            )
+        elif is_cwh(inp):
             # Convert from CWH -> WHC (normalize) -> CWH
             return whc_to_cwh(
                 self.n.transform(
                     cwh_to_whc(inp),
                     augment=augment
                 )
             )
```

## slideflow/norm/torch/macenko.py

```diff
@@ -5,14 +5,15 @@
 """
 
 import torch
 import numpy as np
 from typing import Tuple, Dict, Optional, Union
 from contextlib import contextmanager
 
+from slideflow import log
 import slideflow.norm.utils as ut
 from .utils import clip_size, standardize_brightness
 
 # -----------------------------------------------------------------------------
 
 def dot(a: torch.Tensor, b: torch.Tensor):
     """Equivalent to np.dot()."""
@@ -269,15 +270,21 @@
         HE, C = self._matrix_and_concentrations(img, mask=mask)
 
         # Normalize stain concentrations.
         maxC = torch.stack((torch.quantile(C[0, :], 0.99), torch.quantile(C[1, :], 0.99)))
 
         return HE, maxC, C
 
-    def transform(self, img: torch.Tensor, *, augment: bool = False) -> torch.Tensor:
+    def transform(
+        self,
+        img: torch.Tensor,
+        *,
+        augment: bool = False,
+        allow_errors: bool = True
+    ) -> torch.Tensor:
         """Normalize an H&E image.
 
         Args:
             img (torch.Tensor): Image, RGB uint8 with dimensions W, H, C.
 
         Keyword args:
             augment (bool): Perform random stain augmentation.
@@ -315,19 +322,29 @@
                 self._augment_params['concentrations_stdev']
             )
             maxCRef = maxCRef.to(img.device)
         else:
             maxCRef = self.target_concentrations.to(img.device)
 
         # Get stain matrix and concentrations from image.
-        if self._ctx_maxC is not None:
-            HE, C = self._matrix_and_concentrations(img)
-            maxC = self._ctx_maxC
-        else:
-            HE, maxC, C = self.matrix_and_concentrations(img)
+        try:
+            if self._ctx_maxC is not None:
+                HE, C = self._matrix_and_concentrations(img)
+                maxC = self._ctx_maxC
+            else:
+                HE, maxC, C = self.matrix_and_concentrations(img)
+        except Exception as e:
+            if allow_errors:
+                log.debug(
+                    "Error encountered during normalization. Returning "
+                    f"original image. Error: {e}"
+                )
+                return img
+            else:
+                raise
 
         tmp = torch.divide(maxC, maxCRef)
         C2 = torch.divide(C, tmp[:, None])
 
         # Recreate the image using reference mixing matrix.
         Inorm = self.Io * torch.exp(-HERef.matmul(C2))
         Inorm = torch.clip(Inorm, 0, 255)
```

## slideflow/simclr/simclr/tf2/__init__.py

```diff
@@ -289,14 +289,15 @@
       specified, we will attempt to automatically detect the GCE project from
       metadata
     gcp_project (str): Project name for the Cloud TPU-enabled project. If not
       specified, we will attempt to automatically detect the GCE project from
       metadata
 
   """
+  logging.debug("Building SimCLR dataset")
   if builder is None:
     builder = tfds.builder(args.dataset, data_dir=args.data_dir)
     builder.download_and_prepare()
   num_train_examples = builder.info.splits[args.train_split].num_examples
   num_eval_examples = builder.info.splits[args.eval_split].num_examples
   args.num_classes = builder.info.features['label'].num_classes
 
@@ -313,53 +314,58 @@
   logging.info('# eval steps: %d', eval_steps)
 
   checkpoint_steps = (
       args.checkpoint_steps or (args.checkpoint_epochs * epoch_steps))
 
   topology = None
   if use_tpu:
+    logging.debug("Configuring TPUs")
     if tpu_name:
       cluster = tf.distribute.cluster_resolver.TPUClusterResolver(
           tpu_name, zone=tpu_zone, project=gcp_project)
     else:
       cluster = tf.distribute.cluster_resolver.TPUClusterResolver(args.master)
     tf.config.experimental_connect_to_cluster(cluster)
     topology = tf.tpu.experimental.initialize_tpu_system(cluster)
     logging.info('Topology:')
     logging.info('num_tasks: %d', topology.num_tasks)
     logging.info('num_tpus_per_task: %d', topology.num_tpus_per_task)
     strategy = tf.distribute.TPUStrategy(cluster)
 
   else:
     # For (multiple) GPUs.
+    logging.debug("Configuring distributed dataset with MirroredStrategy")
     strategy = tf.distribute.MirroredStrategy()
     logging.info('Running using MirroredStrategy on %d replicas',
                  strategy.num_replicas_in_sync)
 
   with strategy.scope():
     model = model_lib.SimCLR(**args.model_kwargs)
 
   if args.mode == 'eval':
+    logging.debug("Performing evaluation")
     for ckpt in tf.train.checkpoints_iterator(
         model_dir, min_interval_secs=15):
       result = perform_evaluation(
         model, builder, eval_steps, ckpt, strategy,
         model_dir, cache_dataset, args
       )
       if result['global_step'] >= train_steps:
         logging.info('Eval complete. Exiting...')
         return
   else:
+    logging.debug("Setting up file writer for logs")
     summary_writer = tf.summary.create_file_writer(model_dir)
     if not os.path.exists(model_dir):
         os.makedirs(model_dir)
     with open(os.path.join(model_dir, 'args.json'), "w") as data_file:
       json.dump(args.to_dict(), data_file, indent=1)
     with strategy.scope():
       # Build input pipeline.
+      logging.debug("Setting up distributed dataset")
       ds = data_lib.build_distributed_dataset(builder, args.train_batch_size,
                                               True, args, strategy)
 
       # Build LR schedule and optimizer.
       learning_rate = model_lib.WarmUpAndCosineDecay(
         learning_rate=args.learning_rate,
         num_examples=num_train_examples,
@@ -391,14 +397,15 @@
         ])
       if args.train_mode == 'finetune' or args.lineareval_while_pretraining:
         supervised_loss_metric = tf.keras.metrics.Mean('train/supervised_loss')
         supervised_acc_metric = tf.keras.metrics.Mean('train/supervised_acc')
         all_metrics.extend([supervised_loss_metric, supervised_acc_metric])
 
       # Restore checkpoint if available.
+      logging.debug("Attempting to restore from checkpoint")
       checkpoint_manager = try_restore_from_checkpoint(
           model, optimizer.iterations, optimizer, model_dir, checkpoint_path,
           keep_checkpoint_max=args.keep_checkpoint_max,
           zero_init_logits_layer=args.zero_init_logits_layer)
 
     steps_per_loop = checkpoint_steps
 
@@ -487,14 +494,15 @@
             images, labels = next(iterator)
             features, labels = images, {'labels': labels}
             strategy.run(single_step, (features, labels))
 
       global_step = optimizer.iterations
       cur_step = global_step.numpy()
       iterator = iter(ds)
+      logging.debug("Beginning training")
       while cur_step < train_steps:
         # Calls to tf.summary.xyz lookup the summary writer resource which is
         # set by the summary writer's context manager.
         with summary_writer.as_default():
           train_multiple_steps(iterator)
           cur_step = global_step.numpy()
           checkpoint_manager.save(cur_step)
```

## slideflow/slide/__init__.py

```diff
@@ -875,15 +875,20 @@
                 tile_um=self.tile_um,
             )
             return slide_report
         else:
             log.debug("Skipping slide report")
             return None
 
-    def preview(self, rois: bool = True, **kwargs) -> Optional[Image.Image]:
+    def preview(
+        self,
+        rois: bool = True,
+        thumb_kwargs: Optional[Dict] = None,
+        **kwargs
+    ) -> Optional[Image.Image]:
         """Performs a dry run of tile extraction without saving any images,
         returning a PIL image of the slide thumbnail annotated with a grid of
         tiles that were marked for extraction.
 
         Args:
             rois (bool, optional): Draw ROI annotation(s) onto the image.
                 Defaults to True.
@@ -911,21 +916,25 @@
         if 'show_progress' not in kwargs:
             kwargs['show_progress'] = (self.pb is None)
         generator = self.build_generator(
             dry_run=True,
             deterministic=False,
             **kwargs
         )
+        if thumb_kwargs is None:
+            thumb_kwargs = dict()
         if generator is None:
-            return self.thumb(rois=rois, low_res=True)
+            return self.thumb(rois=rois, low_res=True, **thumb_kwargs)
         locations = []
         for tile_dict in generator():
             locations += [tile_dict['loc']]
         log.debug(f"Previewing with {len(locations)} extracted tile locations.")
-        return self.thumb(coords=locations, rois=rois, low_res=True)
+        return self.thumb(
+            coords=locations, rois=rois, low_res=True, **thumb_kwargs
+        )
 
 
 class WSI(_BaseLoader):
     '''Loads a slide and its annotated region of interest (ROI).'''
 
     def __init__(
         self,
@@ -1789,15 +1798,16 @@
         mpp: Optional[float] = None,
         width: Optional[int] = None,
         coords: Optional[List[int]] = None,
         rois: bool = False,
         linewidth: int = 2,
         color: str = 'black',
         use_associated_image: bool = False,
-        low_res: bool = False
+        low_res: bool = False,
+        **kwargs
     ) -> Image.Image:
         """Returns PIL Image of thumbnail with ROI overlay.
 
         Args:
             mpp (float, optional): Microns-per-pixel, used to determine
                 thumbnail size.
             width (int, optional): Goal thumbnail width (alternative to mpp).
@@ -1832,15 +1842,16 @@
                 roi_scale = self.dimensions[0] / width  # type: ignore
 
         thumb = super().thumb(
             mpp=mpp,
             width=width,
             coords=coords,
             use_associated_image=use_associated_image,
-            low_res=low_res
+            low_res=low_res,
+            **kwargs
         )
 
         if rois and len(self.rois):
             annPolys = [
                 sg.Polygon(annotation.scaled_area(roi_scale))
                 for annotation in self.rois
             ]
```

## slideflow/slide/report.py

```diff
@@ -187,15 +187,16 @@
             tile_um=self.tile_um,
             verbose=False
         )
         self._thumb = wsi.thumb(
             coords=self.thumb_coords,
             rois=self.has_rois,
             low_res=True,
-            width=512
+            width=512,
+            rect_linewidth=1,
         )
         self._thumb = Image.fromarray(np.array(self._thumb)[:, :, 0:3])
 
     def _compress(self, img: bytes) -> bytes:
         with io.BytesIO() as output:
             img = Image.open(io.BytesIO(img))
             if img.height > 256:
```

## slideflow/studio/gui/viewer/_slide.py

```diff
@@ -1,11 +1,12 @@
 """Utility for an efficient, tiled Whole-slide image viewer."""
 
 import imgui
 import numpy as np
+from contextlib import contextmanager
 from typing import Tuple, Optional, TYPE_CHECKING
 from ._viewer import Viewer
 from .. import gl_utils, text_utils
 from ...utils import EasyDict
 
 import slideflow as sf
 
@@ -255,43 +256,47 @@
         ds_level = self.wsi.slide.best_level_for_downsample(p.target_ds)
         region = self.wsi.slide.get_downsampled_image(ds_level)
         resize_factor = self.wsi.slide.level_downsamples[ds_level] / p.target_ds
         region = region.resize(resize_factor)
 
         # Fill in parts of the missing image
         if p.moved_right and p.full_dx:
-            new_horizontal = region.crop(
-                int(p.tl_new[0] / p.target_ds),
-                int(p.tl_new[1] / p.target_ds),
-                p.dx,
-                view_params.target_size[1])
+            left_edge = int(p.tl_new[0] / p.target_ds)
+            top_edge = int(p.tl_new[1] / p.target_ds)
+            extract_w = p.dx
+            extract_h = min(view_params.target_size[1], region.height)
+            with log_vips_error(left_edge, top_edge, extract_w, extract_h):
+                new_horizontal = region.crop(left_edge, top_edge, extract_w, extract_h)
             new_horizontal = self._process_vips(new_horizontal)
             new_view[:, None:p.dx, :] = new_horizontal
         if p.moved_down and p.full_dy:
-            new_vertical = region.crop(
-                int(p.tl_new[0] / p.target_ds),
-                int(p.tl_new[1] / p.target_ds),
-                view_params.target_size[0],
-                p.dy)
+            left_edge = int(p.tl_new[0] / p.target_ds)
+            top_edge = int(p.tl_new[1] / p.target_ds)
+            extract_w = min(view_params.target_size[0], region.width)
+            extract_h = p.dy
+            with log_vips_error(left_edge, top_edge, extract_w, extract_h):
+                new_vertical = region.crop(left_edge, top_edge, extract_w, extract_h)
             new_vertical = self._process_vips(new_vertical)
             new_view[None:p.dy, :, :] = new_vertical
         if p.moved_left and p.full_dx:
-            new_horizontal = region.crop(
-                int((p.tl_new[0] + view_params.window_size[0] + p.full_dx) / p.target_ds),
-                int(p.tl_new[1] / p.target_ds),
-                -p.dx,
-                view_params.target_size[1])
+            left_edge = int((p.tl_new[0] + view_params.window_size[0] + p.full_dx) / p.target_ds)
+            top_edge = int(p.tl_new[1] / p.target_ds)
+            extract_w = -p.dx
+            extract_h = min(view_params.target_size[1], region.height)
+            with log_vips_error(left_edge, top_edge, extract_w, extract_h):
+                new_horizontal = region.crop(left_edge, top_edge, extract_w, extract_h)
             new_horizontal = self._process_vips(new_horizontal)
             new_view[:, view_params.target_size[0]+p.dx:None, :] = new_horizontal
         if p.moved_up and p.full_dy:
-            new_vertical = region.crop(
-                int(p.tl_new[0] / p.target_ds),
-                int((p.tl_new[1] + view_params.window_size[1] + p.full_dy) / p.target_ds),
-                view_params.target_size[0],
-                -p.dy)
+            left_edge = int(p.tl_new[0] / p.target_ds)
+            top_edge = int((p.tl_new[1] + view_params.window_size[1] + p.full_dy) / p.target_ds)
+            extract_w = min(view_params.target_size[0], region.width)
+            extract_h = -p.dy
+            with log_vips_error(left_edge, top_edge, extract_w, extract_h):
+                new_vertical = region.crop(left_edge, top_edge, extract_w, extract_h)
             new_vertical = self._process_vips(new_vertical)
             new_view[view_params.target_size[1]+p.dy:None, :, :] = new_vertical
 
         return new_view
 
     def _read_from_pyramid(self, **kwargs) -> np.ndarray:
         """Read from the whole-slide image pyramid and convert to numpy array.
@@ -596,7 +601,24 @@
         self.view_zoom = new_zoom
         new_origin = [wsi_x - (cx * self.wsi_window_size[0] / self.width),
                       wsi_y - (cy * self.wsi_window_size[1] / self.height)]
 
         view_params = self._calculate_view_params(new_origin)
         if view_params != self.view_params:
             self._refresh_view_full(view_params=view_params)
+
+# -----------------------------------------------------------------------------
+
+@contextmanager
+def log_vips_error(left_edge, top_edge, extract_w, extract_h):
+    try:
+        yield
+    except Exception:
+        sf.log.error(
+            "Error attempting to crop pyvips image, with "
+            "top/left (x,y) = ({}, {}) and width/height = ({}, {})".format(
+                left_edge,
+                top_edge,
+                extract_w,
+                extract_h
+            ))
+        raise
```

## slideflow/util/__init__.py

```diff
@@ -936,14 +936,15 @@
     """
 
     import matplotlib.pyplot as plt
     import matplotlib.colors as mcol
 
     slide_name = sf.util.path_to_name(slide)
     log.info(f'Generating heatmap for [green]{slide}[/]...')
+    log.debug(f"Plotting {len(values)} values")
     wsi = sf.slide.WSI(slide, tile_px, tile_um, verbose=False)
 
     stats = {}
 
     # Loaded CSV coordinates:
     x = locations[:, 0]
     y = locations[:, 1]
```

## slideflow/util/tfrecord2idx.py

```diff
@@ -236,16 +236,18 @@
     if 'image_raw' in record:
         record['image_raw'] = bytes(record['image_raw'])
     if 'loc_x' in record:
         record['loc_x'] = record['loc_x'][0]
     if 'loc_y' in record:
         record['loc_y'] = record['loc_y'][0]
 
+    file.close()
     return record
 
+
 def process_record(record, description=None):
     if description is None:
         description = FEATURE_DESCRIPTION
     example = sf.util.example_pb2.Example()
     example.ParseFromString(record)
     return sf.util.extract_feature_dict(
         example.features,
```

## Comparing `slideflow-2.0.2.post1.data/scripts/slideflow-studio` & `slideflow-2.0.3.data/scripts/slideflow-studio`

 * *Files identical despite different names*

## Comparing `slideflow-2.0.2.post1.data/scripts/slideflow-studio.py` & `slideflow-2.0.3.data/scripts/slideflow-studio.py`

 * *Files identical despite different names*

## Comparing `slideflow-2.0.2.post1.dist-info/LICENSE` & `slideflow-2.0.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `slideflow-2.0.2.post1.dist-info/METADATA` & `slideflow-2.0.3.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: slideflow
-Version: 2.0.2.post1
+Version: 2.0.3
 Summary: Deep learning tools for digital histology
 Home-page: https://github.com/jamesdolezal/slideflow
 Author: James Dolezal
 Author-email: james.dolezal@uchospitals.edu
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: GNU General Public License v3 (GPLv3)
 Classifier: Operating System :: OS Independent
@@ -45,43 +45,43 @@
 Requires-Dist: glfw
 Requires-Dist: saliency
 Requires-Dist: pyperclip
 Requires-Dist: requests
 Requires-Dist: parameterized
 Requires-Dist: zarr
 Provides-Extra: all
-Requires-Dist: cellpose ; extra == 'all'
+Requires-Dist: cellpose (<2.2) ; extra == 'all'
 Requires-Dist: cucim ; extra == 'all'
 Requires-Dist: sphinx ; extra == 'all'
 Requires-Dist: sphinx-markdown-tables ; extra == 'all'
 Requires-Dist: sphinxcontrib-video ; extra == 'all'
 Requires-Dist: torch ; extra == 'all'
 Requires-Dist: torchvision ; extra == 'all'
 Requires-Dist: fastai ; extra == 'all'
 Requires-Dist: pretrainedmodels ; extra == 'all'
 Requires-Dist: tensorflow (<2.12,>=2.7) ; extra == 'all'
 Requires-Dist: tensorflow-probability (<0.20) ; extra == 'all'
 Requires-Dist: tensorflow-datasets ; extra == 'all'
 Provides-Extra: cellpose
-Requires-Dist: cellpose ; extra == 'cellpose'
+Requires-Dist: cellpose (<2.2) ; extra == 'cellpose'
 Provides-Extra: cucim
 Requires-Dist: cucim ; extra == 'cucim'
 Provides-Extra: dev
 Requires-Dist: sphinx ; extra == 'dev'
 Requires-Dist: sphinx-markdown-tables ; extra == 'dev'
 Requires-Dist: sphinxcontrib-video ; extra == 'dev'
 Provides-Extra: tf
 Requires-Dist: tensorflow (<2.12,>=2.7) ; extra == 'tf'
 Requires-Dist: tensorflow-probability (<0.20) ; extra == 'tf'
 Requires-Dist: tensorflow-datasets ; extra == 'tf'
 Provides-Extra: torch
 Requires-Dist: torch ; extra == 'torch'
 Requires-Dist: torchvision ; extra == 'torch'
 Requires-Dist: pretrainedmodels ; extra == 'torch'
-Requires-Dist: cellpose ; extra == 'torch'
+Requires-Dist: cellpose (<2.2) ; extra == 'torch'
 Requires-Dist: fastai ; extra == 'torch'
 
 ![slideflow logo](https://github.com/jamesdolezal/slideflow/raw/master/docs-source/pytorch_sphinx_theme/images/slideflow-banner.png)
 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5703792.svg)](https://doi.org/10.5281/zenodo.5703792)
 [![Python application](https://github.com/jamesdolezal/slideflow/actions/workflows/python-app.yml/badge.svg?branch=master)](https://github.com/jamesdolezal/slideflow/actions/workflows/python-app.yml)
 [![PyPI version](https://badge.fury.io/py/slideflow.svg)](https://badge.fury.io/py/slideflow)
```

## Comparing `slideflow-2.0.2.post1.dist-info/RECORD` & `slideflow-2.0.3.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 slideflow/__init__.py,sha256=Y463ep8Pwe89TUUVaqmFAbKmtQVjjDEEZHIlQ8unTE0,1411
 slideflow/_backend.py,sha256=Yi7LUgiYUUVlGdi6DBYe2raev3LLdYdtR6PtHozFORU,1662
-slideflow/_version.py,sha256=nImgDnSmhmhIiq0A857AmkLhyAQlOxmLHvJpvKxDRp0,503
-slideflow/dataset.py,sha256=WH9YbMhBPI0qHL218Py96RvLV2m_fg9WrnNNsOpOjVQ,159745
+slideflow/_version.py,sha256=cVAoiS7t4rmlIrdnG9c1v5uYZFiUPO-J5yF5EuTk1vo,497
+slideflow/dataset.py,sha256=qWnY-2SB1iAYEIKiDefgAiif7nIfAdZCnpWhCJdOQmg,159998
 slideflow/errors.py,sha256=ueMKAc73Xz1OoSzibSt_qewJ29Lkk8KjkOkaNMw5ks0,3733
 slideflow/heatmap.py,sha256=7p6RenakRi01yo6DGH5mIUV9MfLQ66Q1iBPV6gnDV0Q,38304
 slideflow/mosaic.py,sha256=GjceACaGgxo2UKqa4AA0RSNaiGTMvEgW0ceFPUpjkrA,25615
 slideflow/project.py,sha256=do9n8psKyemUyWQ7nY1yl_ofI1JtZDB8kYrjIjga8YQ,168044
-slideflow/project_utils.py,sha256=YzjnuM3faru-lqQau7rBhuCyXGSGIXpSAmkR3hzCAio,33320
+slideflow/project_utils.py,sha256=ollersimiyYZC6VYRBvom_Q4Ij15Ku18BtRwEFMuzfw,33332
 slideflow/sample_actions.py,sha256=khhho6m0GzAXUD9licPFKlq8oxqY3hdP1qg1B3CTOMY,978
 slideflow/biscuit/__init__.py,sha256=iSjHWIB2GZloqeMxPhNSpb90gBagYm_hX7xut8rKf48,1195
 slideflow/biscuit/delong.py,sha256=3zc0ctK1ZVHaeyFfxOzkKU8doNlzbfPiNkeA18moCs0,4281
 slideflow/biscuit/errors.py,sha256=nAGXgZnJT6GAUs0zw6tNZ3jCfPM_7U3rcu1roSWe1zA,318
 slideflow/biscuit/experiment.py,sha256=SNrkbYkrOlX_-Pf1LY7cuG2GcX8UYvrhbVhcPkOr5Gs,46729
 slideflow/biscuit/hp.py,sha256=bSuup8s4G9mxpSZ0Wy1ADlpmsNfbBsXzv1dmspAOPGU,1260
 slideflow/biscuit/threshold.py,sha256=k7Of9UwlU4HJjsAGcAwepoanNptFql1bdVhjFfVtE5Q,21373
@@ -153,47 +153,47 @@
 slideflow/gan/stylegan3/stylegan3/viz/trunc_noise_widget.py,sha256=j8Is6WXvSx9iOgQdKfw2zC_V7wPPX3xBjak0gUegTH8,3845
 slideflow/grad/__init__.py,sha256=_cBjPmYSKKW6urH2xZ6llP-gthQoKCsgxfx0Lksq7cs,15662
 slideflow/grad/plot_utils.py,sha256=vkUAan14mAY3UoMCw5oqtgfiifqWcSw5BPYuD9yDIE4,7376
 slideflow/io/__init__.py,sha256=xnHhQDCnsX3cNzz2YvGtob4cG_mo6B6z7JFXcHSNFF0,11715
 slideflow/io/gaussian.py,sha256=7mhQeBcEQmpKqq_55WST6VE67jQwFf2uq6u7ZVM99PM,10541
 slideflow/io/io_utils.py,sha256=8oneye-50sEu9eZpaCzy2IdkrCGBd3WKlVSA4s8mW9A,9918
 slideflow/io/tensorflow.py,sha256=R52b44ZkKdUUoXHGDh02v28pV1fJ07EJ1LhmYKQQRi0,34511
-slideflow/io/torch.py,sha256=OXUtAVqEC45CXsXREMmD7EVl2aCAC90P9zVWpGtYXWI,44556
+slideflow/io/torch.py,sha256=AD-9UXRWlHWXaGt8sgblar7gp9nZ5M1Jow6nMkSw7iQ,45085
 slideflow/io/preservedsite/__init__.py,sha256=9chqMmsn_iKvFwnTxZtVr4Pn08dyOeEi4YpgKhrf1Kw,68
 slideflow/io/preservedsite/crossfolds.py,sha256=UD4e0JqgZJeYl9lfrFHkCKhePJHGbsB20ZK6CA3g7bU,8419
 slideflow/mil/__init__.py,sha256=H_nvcxN4HLMS_rnghdaBTqeaG6gb-7sJctm0UVz38Fc,284
 slideflow/mil/_params.py,sha256=sCpjitfJxCcsx7iTnOHqD2cLXJe0Z-qxzcldUTYWgBI,14819
 slideflow/mil/data.py,sha256=DIUUAPhAk0qthJQT6Oe4cfIxJmR5BXKPTbnXzH_sKoo,5160
-slideflow/mil/eval.py,sha256=QQkRaAMUGUZcPNPaajMV3sXNBZr4-6IMjqTlsRjbO8Q,15565
+slideflow/mil/eval.py,sha256=I5nqtWM-NzxVt5QboL5hU8nhEMcC1AgWfa5oYS6Xi-g,15565
 slideflow/mil/clam/__init__.py,sha256=XFJYtb4FK5xQm7DqdmW3gBnG8IHYEjh3S1pzwFcbDKE,3890
 slideflow/mil/clam/create_attention.py,sha256=oGjZEfcT0zRqMhB8XITtFXHdw5Y9lNT9ZeiOcjntFtE,4334
 slideflow/mil/clam/datasets/__init__.py,sha256=lrqpfKa8fvcZHBzpmdJWt1uikjTNV8ZKqqpMIoRJkh4,1131
 slideflow/mil/clam/datasets/dataset_generic.py,sha256=Ulj-XQigidFsdx_DAoJ-RTQnY3jZucDtZH3YJ95Sw4g,14990
 slideflow/mil/clam/utils/__init__.py,sha256=EbG-maRA3elGYWTGMzVZbQfRqf2p9I8ojMHHJkb1MgU,5914
 slideflow/mil/clam/utils/core_utils.py,sha256=hWgMxfLv4gRWwyBzS0vhDRGJ4IF_AUz6IA6SedbII7U,19782
 slideflow/mil/clam/utils/eval_utils.py,sha256=zFFeNXOP-FhUg2HrYJxG_k5BUFwso1tJo_-kpIxAmWY,4150
 slideflow/mil/clam/utils/file_utils.py,sha256=uj7pGXXTKrB4MeLc2IGHO3C6UZ9980H10-nppd4gOFc,1287
 slideflow/mil/clam/utils/loss_utils.py,sha256=crw4fumbNZxcKWYCVBKMSZS6Ig3Cfr_YWfCZpC2rU3A,3497
 slideflow/mil/models/__init__.py,sha256=KbHLo_IiQOJ6wkOarvVur2kvJY2L38xmccgAYJTJOnk,185
 slideflow/mil/models/_utils.py,sha256=OzeCy0V1sY4wzcEI-4XgWa6vZ_knIH8lr5_JvosPQJM,378
 slideflow/mil/models/att_mil.py,sha256=Tor-RDpdQ1OmXo3pd_7wTT3-qZZRxVtIC3EQxsDROPg,3239
 slideflow/mil/models/clam.py,sha256=7XiRuX4dnkaevi_0WYaaTky98xwkmCO9jtnsXgtfuy0,12816
-slideflow/mil/models/mil_fc.py,sha256=xZ7t5P--bzbnJscs5iTUwxkyStbgkDnXzLB2c862HmE,3607
+slideflow/mil/models/mil_fc.py,sha256=TOQdoVmuyf-wiNJgyAvL2MubdcyDFamEO0GHMkNeeHI,3865
 slideflow/mil/models/transmil.py,sha256=CpVxNifuytkGE7oRUcc2nFWJQHWo51xGXHtn8BIvk7A,4137
 slideflow/mil/train/__init__.py,sha256=SNILpMx3nYeBFYmO2Y_PoJAiC0LgdGKRAKD_pE052ec,15008
 slideflow/mil/train/_fastai.py,sha256=Af8wZL5gOaoKYq8helUE5SpfuxfcdZ8XxB24p4XXFTA,8214
 slideflow/mil/train/_legacy.py,sha256=v1s5Gqc46BZggwz7dcNyLuqVIM9lIYgUfNqTfH6d90A,7795
 slideflow/model/__init__.py,sha256=oJpmQVOsQiDUhI7RqD7WC3n5dKxkrOSA5Emkfgs-Vf0,7485
 slideflow/model/adv_utils.py,sha256=qt25QlPxEQk7vJtqc82DBlkw9KgSf8KkJvpC0ATzF7s,1879
 slideflow/model/base.py,sha256=0tOogk4fN9FMkrJCK5K0lgh3_vVI6KQIrfgMKBPQtg4,23588
-slideflow/model/features.py,sha256=Y4f6GV3anOUNzsMVgBwRo2ct5ih-eRxs8oqIqgL54EI,55263
+slideflow/model/features.py,sha256=_iiSEho53mP1F4u2f8qNI9_6LuyvEVFy9oXa4wiIU4c,55334
 slideflow/model/tensorflow.py,sha256=OY3JY_UK5ZIPdHGkjzQJfGE141LsWeQbaSWP7WMIYHg,110908
 slideflow/model/tensorflow_utils.py,sha256=5HYrVE8Ph3pSXjMfSRi7vmJIC_8mD3pQd28LLj1YAEY,22547
-slideflow/model/torch.py,sha256=TwJ0pgg_ZOyfT8EZXTBTjSfNoxrI4YhQxVnMrsIbidw,103179
-slideflow/model/torch_utils.py,sha256=Zudk3htcyZ1wnvZRX8z8xJsnwkS8M8n63vdqBQXaf54,16945
+slideflow/model/torch.py,sha256=vwjNP-xIB7MPpmCsjrgBLhckJ0M2OKYMaL6F1MlDFUo,103271
+slideflow/model/torch_utils.py,sha256=wFUA6VgDydOFVqZj-pWRXlp7PPbdV7-W_ZlMi9rszSU,17005
 slideflow/model/extractors/__init__.py,sha256=MEt448aADc_WUGzTSGF5IvW4Neu2atSiAFAyxzc-4uI,394
 slideflow/model/extractors/_factory.py,sha256=CXR4fa08GLM8Q4b_S5qIO1bmtzRX1VDtYrU7PjPFUHI,3708
 slideflow/model/extractors/_factory_tensorflow.py,sha256=zpvP7rOYQEvbuZbSlFY_L7iV0dub53kHS8fAcYNnaHs,4544
 slideflow/model/extractors/_factory_torch.py,sha256=-5kOiu91eBWq_9erNSTIj7DiUYg4edVKl5SPns0_9JQ,5711
 slideflow/model/extractors/_registry.py,sha256=BBudKPe2sOGvnNZFiuFJhjcKhHZdf2gRe0hheTN8ZKo,1570
 slideflow/model/extractors/_slide.py,sha256=nC0OzgF8_11f5ql0oujeVymM8X8DJ6HU71C4ts-Q9s8,2646
 slideflow/model/extractors/ctranspath.py,sha256=fvxFcJFnd0E5KRRlUgQleXwRNfEMpnsncLLzQKRlgrw,25681
@@ -206,33 +206,33 @@
 slideflow/norm/utils.py,sha256=dA9JzmAhz1tnzKoCBAgbTPX6iLNhfm6phd9Ng5xWrz0,15314
 slideflow/norm/vahadane.py,sha256=YIRyky8TCVqT_H95wijSbwgOPoOMhH9bApKABypLP30,7836
 slideflow/norm/tensorflow/__init__.py,sha256=EKJBEUXsly7J1j_Z5byskYVpQJzMXUXsOWMpJjrq3lM,11998
 slideflow/norm/tensorflow/color.py,sha256=ei3alBtNb-xm1pHhJdtJoVuS_tPkISh1rXZ6Sl_DvwM,5879
 slideflow/norm/tensorflow/macenko.py,sha256=J4jfuDBJUWpxriY8uPg23QDZyNfYh7Lsurn1j_eTOTE,20759
 slideflow/norm/tensorflow/reinhard.py,sha256=w0DsrQx7bq57u7HpC5bgQZBjOlqNsHOnEfQrfXfcASU,26169
 slideflow/norm/tensorflow/utils.py,sha256=IcTPD8wR5h8Gqso0O-d6mlR-LrU6d1cJdnkj62XIHm0,1685
-slideflow/norm/torch/__init__.py,sha256=cybFq40B8M0u0-biqEFdvyideiutxhm0qbC8Qnu0WXs,11313
+slideflow/norm/torch/__init__.py,sha256=xigN3UHKRzO_--2eUzfmKi9F6MgV5rSMuJErCzVt14c,12095
 slideflow/norm/torch/color.py,sha256=dN1FpcvdVLvmkjGdc77rA4r0q0Clhhr_sQR2fEh18Vc,7826
-slideflow/norm/torch/macenko.py,sha256=BpVulIdIXUhaUrJnFgHBF1W4jj2ZVTXB4lDiSSkxtEo,14869
+slideflow/norm/torch/macenko.py,sha256=OzwzBmbTNjk5fIT_6PxsPsHAxY4J2YkSvsrFUYetPMM,15296
 slideflow/norm/torch/reinhard.py,sha256=xE74ZJQTDJckRgumwNikRy3UT8KDq1EoumHO3x3BaVc,24997
 slideflow/norm/torch/utils.py,sha256=MstCD6KIJGG2a7xHAfSpLVr95jcoKrXrW2xDxch79pQ,1673
 slideflow/simclr/__init__.py,sha256=o8TiOkbZzvsQo6bISoejBrOscbq_l-akGzbGr92rkK4,458
 slideflow/simclr/simclr/__init__.py,sha256=M1HPoNrZnhd3zOIQyYO1YXGBeA22ZXmi6BeSccg5774,6
-slideflow/simclr/simclr/tf2/__init__.py,sha256=H_209JGDuRkTGk0kJe1kRgUTg26KyU1AfoEQXpUhV3Q,20637
+slideflow/simclr/simclr/tf2/__init__.py,sha256=_z4lp94Onev6T60Ny_ScPHYZoLMoNhuwCxxBNkjSXdo,21046
 slideflow/simclr/simclr/tf2/data.py,sha256=wKVweVOUpusJfo9lkGpN5lo_xiDOr-lUfjBtZIlymzA,10701
 slideflow/simclr/simclr/tf2/data_util.py,sha256=wu0vlo_H_yL8aFCWWGpknf1Z3Apu3gsiyaiEGaEPcRY,18550
 slideflow/simclr/simclr/tf2/lars_optimizer.py,sha256=XWLnxqVxRlUY6PsQOHiCsFB6Lluk3TOokasLH9TTO-o,6505
 slideflow/simclr/simclr/tf2/metrics.py,sha256=RUtmvVamcnS-8_ZXLE2XdALsLgNgVPOUNl17hKPk13Y,2997
 slideflow/simclr/simclr/tf2/model.py,sha256=3p7YnZeZS09XOkPbVJ19dl77h7w3DNuf4c6rd6gqlqA,12256
 slideflow/simclr/simclr/tf2/objective.py,sha256=v5V1UzGCaSzT1i6xDe4UOaUIOhQyZ9sPDlrEP_2XYQ8,4983
 slideflow/simclr/simclr/tf2/resnet.py,sha256=BJgzhGO3TudNubzAAsx3jl8wsUW6DJ8EzivN3qlV9wY,28397
 slideflow/simclr/simclr/tf2/run.py,sha256=8Ej3YtqrKgpqlh0sjyqbUmKvLPGIRLz7XYi44dpvPE0,6524
 slideflow/simclr/simclr/tf2/utils.py,sha256=BEiKTWmuJ0XbZfG15413xPs_KnNmRg5uYL0QZEzgDYo,9517
-slideflow/slide/__init__.py,sha256=t5oi-QVeUhSF8PsGuxxCYy0DF1S8yAUKqmDpeXgw9Pw,114717
-slideflow/slide/report.py,sha256=A0GEqPN5MisFk9gWuAsFq0qRBuUd1NccZgnHM7UyNHU,19045
+slideflow/slide/__init__.py,sha256=V9AzoJOlJnvihmy_wM4spLRWeK6-g_uNLPwQJCrjnQQ,114953
+slideflow/slide/report.py,sha256=4vAxYiLaqHaoLSeLFLxZmSbIhg7Hv3UbhPhQk1xuC6g,19076
 slideflow/slide/slideflow-logo-name-small.jpg,sha256=C9-2QV_cZkmn_FqzvorF5GvrkD__VE-7NSKME58fTR8,30934
 slideflow/slide/utils.py,sha256=5NmVU1LdFdBciwJLg_Sh9xMU4DgXEVKEB-XV7jBf2lc,5516
 slideflow/slide/backends/__init__.py,sha256=9BjTu2AaVW8zySEvLrZ-RQ21ET9kqSajYV8YkCMpAzU,708
 slideflow/slide/backends/cucim.py,sha256=dcQzgy8BzktiOY2v8t_EJn35X7OG4orC1RMWPv-qRZ4,14008
 slideflow/slide/backends/vips.py,sha256=VbBwd4S8q2-YyXl_56Nb12E8XW5-u8gWu43jXo6YN9A,22391
 slideflow/slide/qc/__init__.py,sha256=ePMfd6ZlrWdxYiRkS6gsCIMoGfS3hDPuqIyjX7kQT30,117
 slideflow/slide/qc/deepfocus_qc.py,sha256=TuHOVLYPWUD_8hYS0OFzmLvgDNAoxxuykeEcb4_xuKc,1010
@@ -305,15 +305,15 @@
 slideflow/studio/gui/icons/error.png,sha256=XQOSYuK4Gipt36VpGPGYse7HbnPnxoqi5oLKxmNvFeU,7568
 slideflow/studio/gui/icons/info.png,sha256=vqiqW4eXPfcrj_m7FyKajAq8r3YI3AkDGzhUp0__jG8,7286
 slideflow/studio/gui/icons/logo.png,sha256=1Vw6Zi1rlfhKtIrN3rOvzKppRnaiaXorYVk-GQPA72s,125763
 slideflow/studio/gui/icons/success.png,sha256=BxXVANTnqknCN79rZabK-gBap_3YrCeCvar8Fxxr0Jc,7181
 slideflow/studio/gui/icons/warn.png,sha256=mAVKiVI85LF10roGtrrOf43DE5gpR4XORYh3z62iACU,6817
 slideflow/studio/gui/viewer/__init__.py,sha256=9wiXSrhD8Raz5AVGdCBrF-Fykn1sNqkqGgjfXtRwNuU,107
 slideflow/studio/gui/viewer/_mosaic.py,sha256=dKDyGt12-f9sju5g0plSJUoOiP4uYAH2emiK3vQFUb0,7874
-slideflow/studio/gui/viewer/_slide.py,sha256=EaU5UA6486fjedJFAwiTUpXgz1Dm061s3OVDsMTSNC4,24832
+slideflow/studio/gui/viewer/_slide.py,sha256=JZHy-MzIOBxn6gEyMcoNx3CmXnplYsiOWirt_2rY5CQ,26041
 slideflow/studio/gui/viewer/_viewer.py,sha256=M_aVUL7tZRyOI2OhI3xiRUVpQe3kunoLAt5kfRMxZSM,14452
 slideflow/studio/widgets/__init__.py,sha256=M8gSPhj8W4FThgSm5kMF1GuAffF1PkauBfU5xtwAmxI,439
 slideflow/studio/widgets/_utils.py,sha256=jI9Ah-aO4SPN8CdBB0pWV2-bxifLSW6DpGK106938js,735
 slideflow/studio/widgets/capture.py,sha256=MtXvouNKrMcxfmHVN4xBv1ODK-clnC3-QzH8waowHmc,4264
 slideflow/studio/widgets/extensions.py,sha256=V7u_T1t5IxiqRfiza9ru5UgvPBUruvLI7dXmTBwlnm8,5412
 slideflow/studio/widgets/heatmap.py,sha256=P_7Bpz3T9MxxFBCeA9moD0TcLRkrbJvV_1QlRVbRriY,16890
 slideflow/studio/widgets/layer_umap.py,sha256=Dol3NIxJge6Ff2e80Ydjvfh0xc8CBMb34DmM-n1lPmg,3842
@@ -339,21 +339,21 @@
 slideflow/tfrecord/__init__.py,sha256=YsF14xnyOnYgR2CCaj6zl0_YSeF7z3CqxZ0yO11dll0,891
 slideflow/tfrecord/iterator_utils.py,sha256=IFRjADsWLpfN65GSZp_1F2CEcgPRw77FszXHNcSegvs,2905
 slideflow/tfrecord/reader.py,sha256=vGf6X-5JRMeUQN1gDDvfXJ131QbHEHhK16bq8CDN_Bw,15505
 slideflow/tfrecord/writer.py,sha256=47JbvdMp9xcx3RgacsIXZ0MlDm2bjREIYWG-XlcFzpQ,5637
 slideflow/tfrecord/tools/__init__.py,sha256=3082iuyLMnQ8Gc2WFwulW1sSJwBqIJPnBNoMT-VuW-o,179
 slideflow/tfrecord/torch/__init__.py,sha256=zl9XVfdCnojlhGDx0mwtqCbjMyj1ypROP-Ut2J_M7eQ,310
 slideflow/tfrecord/torch/dataset.py,sha256=Vc9q1vHG0xTtMu-tsi-kX1c9wM95BNWb9r5CWzh8B0w,7857
-slideflow/util/__init__.py,sha256=xaoOXySQBk5xPlk7arNDV1ZAp4ohvvh0dsBw6KNLiOg,41104
+slideflow/util/__init__.py,sha256=99O7uXX9F0-PBuaKDooHShPhUgnw7p1kHSt31rl717I,41152
 slideflow/util/colors.py,sha256=KOfggJhBNe3uHYa4MNQYdJnOb3A-Nyl1lxIMQ6Sqivc,738
 slideflow/util/example_pb2.py,sha256=oU4oQBXz89HAyrehrDeK-uJYqL7eNznjd8y3YDwn0dY,17912
 slideflow/util/log_utils.py,sha256=kyfTOCmSJW7gIW_1nHXiyPydN-jyO8YEBM3wiEb5OUc,4468
 slideflow/util/neptune_utils.py,sha256=rus5wDerStaFQalTbF9VaEbi6OelhkpkUHmwt0ggejQ,4381
 slideflow/util/smac_utils.py,sha256=T_-b1Wv0V2fQVX-FGlnhX1i4bu69imixlbAJd2aNJgs,20061
-slideflow/util/tfrecord2idx.py,sha256=RADRWAR2J4YJITIsaIHoNxsGJXbTkKQqs9xtW2kxXJ0,8046
-slideflow-2.0.2.post1.data/scripts/slideflow-studio,sha256=BljDc6Eig-Hl5HLA41v-5VENDbTTkmHtK-mUAGHM1Ro,14085
-slideflow-2.0.2.post1.data/scripts/slideflow-studio.py,sha256=dYg_ktDH_I74oIs8Sj4l56M-03frzO4KisE7WeXAofI,14085
-slideflow-2.0.2.post1.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
-slideflow-2.0.2.post1.dist-info/METADATA,sha256=PqJxr-P_8Nqik2sLsQqkRrl9ukU7d8_WRtafosRk5pk,13004
-slideflow-2.0.2.post1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-slideflow-2.0.2.post1.dist-info/top_level.txt,sha256=FRikcoh3_TcsLUYbI4LowF3nrEDWcGLaAyNgsi9Lu9M,10
-slideflow-2.0.2.post1.dist-info/RECORD,,
+slideflow/util/tfrecord2idx.py,sha256=wQjewrIjxXrC3Z9Zp9-jfjrPR-WOHC3lohoftSrxtEk,8064
+slideflow-2.0.3.data/scripts/slideflow-studio,sha256=BljDc6Eig-Hl5HLA41v-5VENDbTTkmHtK-mUAGHM1Ro,14085
+slideflow-2.0.3.data/scripts/slideflow-studio.py,sha256=dYg_ktDH_I74oIs8Sj4l56M-03frzO4KisE7WeXAofI,14085
+slideflow-2.0.3.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
+slideflow-2.0.3.dist-info/METADATA,sha256=I6SNgbgNnSJzlJRdTdQpyOUHfaD2vRkpG6sOuNbmOPU,13019
+slideflow-2.0.3.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+slideflow-2.0.3.dist-info/top_level.txt,sha256=FRikcoh3_TcsLUYbI4LowF3nrEDWcGLaAyNgsi9Lu9M,10
+slideflow-2.0.3.dist-info/RECORD,,
```

